{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Load_dataset_TPU_GPU_WITH_UTILS.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOH+1u3az0Q7rdqk/4trh1W"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"KKt1zff7HU0g","colab_type":"code","colab":{}},"source":["#  UTILS\n","\n","def display_9_images_from_dataset(dataset):\n","  plt.figure(figsize=(13,13))\n","  subplot=331\n","  for i, (image, label) in enumerate(dataset):\n","    plt.subplot(subplot)\n","    plt.axis('off')\n","    plt.imshow(image.numpy().astype(np.uint8))\n","    subplot += 1\n","    if i==2:\n","      break\n","  plt.tight_layout()\n","  plt.subplots_adjust(wspace=0.1, hspace=0.1)\n","  plt.show()\n","\n","def resize_and_crop_image(image, label):\n","    w = tf.shape(image)[0]\n","    h = tf.shape(image)[1]\n","    tw = TARGET_SIZE[1]\n","    th = TARGET_SIZE[0]\n","    resize_crit = (w * th) / (h * tw)\n","    image = tf.cond(resize_crit < 1,\n","                    lambda: tf.image.resize(image, [w*tw/w, h*tw/w]), # if true\n","                    lambda: tf.image.resize(image, [w*th/h, h*th/h])  # if false\n","                   )\n","    nw = tf.shape(image)[0]\n","    nh = tf.shape(image)[1]\n","    image = tf.image.crop_to_bounding_box(image, (nw - tw) // 2, (nh - th) // 2, tw, th)\n","    return image, label\n","\n","def normalize(image, label):\n","    image = tf.image.per_image_standardization(image)\n","    return image, label\n","\n","\n","def augmentation(image, label):\n","    crop_w = int(TARGET_SIZE[0]*0.95) \n","    crop_h = int(TARGET_SIZE[1]*0.95)\n","    crop_or_pad_s = int(TARGET_SIZE[0]*0.1) + TARGET_SIZE[0]\n","    crop_or_pad_p = int(TARGET_SIZE[1]*0.1) + TARGET_SIZE[1]\n","    image = tf.image.resize_with_crop_or_pad(image, crop_or_pad_s, crop_or_pad_p)\n","    image = tf.image.random_crop(image, [crop_w, crop_h, 3])\n","    image = tf.image.random_flip_left_right(image)\n","    image = tf.image.random_flip_up_down(image)\n","    image = tf.image.rot90(image, k=random.randrange(4))\n","    image = tf.image.random_saturation(image, 0.6, 1.6)\n","    image = tf.image.random_hue(image, 0.08)\n","    image = tf.image.random_contrast(image, 0.7, 1.3)\n","    image = tf.image.random_brightness(image, 0.05)\n","    return image, label\n","\n","def count_data_items(filenames):\n","    return np.sum([int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames])\n","\n","def force_image_sizes(dataset):\n","    reshape_images = lambda image, label: (tf.reshape(image, SHAPE), label)   \n","    dataset = dataset.map(reshape_images, num_parallel_calls=AUTO)\n","    return dataset\n","\n","## READ_RECORD\n","def read_tfrecord(example):\n","    features = {\n","      \"image\":tf.io.FixedLenFeature([], tf.string), \n","      \"label\":tf.io.FixedLenFeature([], tf.string),          \n","      \"head_root_hot\":tf.io.VarLenFeature(tf.float32),\n","      \"head_vowel_hot\":tf.io.VarLenFeature(tf.float32),\n","      \"head_consonant_hot\": tf.io.VarLenFeature(tf.float32)\n","     }\n","    # decode the TFRecord\n","    example        = tf.io.parse_single_example(example, features)\n","#     image          = tf.image.decode_jpeg(example['image'], channels=3)\n","#     image          = tf.cast(image, tf.float32)\n","#     image = tf.reshape(image, [*TARGET_SIZE, 3])\n","    image = tf.image.decode_jpeg(example['image'], channels=3)\n","    label          = example['label']\n","    head_root      = tf.sparse.to_dense(example['head_root_hot'])\n","    head_vowel     = tf.sparse.to_dense(example['head_vowel_hot'])\n","    head_consonant = tf.sparse.to_dense(example['head_consonant_hot'])\n","  \n","    return image,{\"head_root\": head_root, \"head_vowel\": head_vowel, \"head_consonant\": head_consonant }\n","\n","# LOAD DATASETS\n","option_no_order = tf.data.Options()\n","option_no_order.experimental_deterministic = False\n","\n","\n","def load_dataset(filenames):\n","    ignore_order = tf.data.Options()\n","    ignore_order.experimental_deterministic = False\n","    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) \n","    dataset = dataset.with_options(ignore_order)\n","    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTO)\n","#     dataset = force_image_sizes(dataset)\n","    return dataset\n","\n","def get_training_dataset():\n","    dataset = load_dataset(TRAINING_FILENAMES)\n","    dataset = dataset.shuffle(BATCH_SIZE)\n","    dataset = dataset.map(augmentation)\n","    dataset = dataset.map(normalize)\n","    dataset = dataset.map(resize_and_crop_image, num_parallel_calls=AUTO) \n","    dataset = dataset.repeat(3)\n","    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n","    dataset = dataset.prefetch(AUTO)\n","    return dataset\n","\n","def get_validation_dataset():\n","    dataset = load_dataset(VALIDATION_FILENAMES)\n","    dataset = dataset.map(resize_and_crop_image, num_parallel_calls=AUTO) \n","    dataset = dataset.batch(VALIDATION_BATCH_SIZE, drop_remainder=True)\n","    dataset = dataset.prefetch(AUTO) \n","    return dataset\n","\n","\n","\n","dataset = tf.data.Dataset.from_tensor_slices((placeholder_X, placeholder_y))\n","dataset = dataset.batch(batch_size)\n","iterator = dataset.make_initializable_iterator()\n","\n","data_X, data_y = iterator.get_next()\n","data_y = tf.cast(data_y, tf.int32)\n","model = Model(data_X, data_y)"],"execution_count":0,"outputs":[]}]}